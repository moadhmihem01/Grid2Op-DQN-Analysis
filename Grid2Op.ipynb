{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roe1ldxSmR4_"
      },
      "outputs": [],
      "source": [
        "# STEP 1: Setup Environment\n",
        "!pip install numpy==1.26.4 --quiet\n",
        "!pip install scipy==1.13.1 --quiet\n",
        "\n",
        "# Install primary RL and simulation packages\n",
        "!pip install stable-baselines3[extra] grid2op l2rpn-baselines --quiet\n",
        "print(\"✓ Core RL/simulation libraries installed.\")\n",
        "\n",
        "# Install analysis and plotting packages\n",
        "!pip install matplotlib shap pandas seaborn --quiet\n",
        "print(\"✓ Analysis/plotting libraries installed.\")\n",
        "\n",
        "# Project Imports\n",
        "import copy\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "\n",
        "import grid2op\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import shap\n",
        "from grid2op.Agent import DoNothingAgent, RandomAgent\n",
        "from grid2op.MakeEnv import make\n",
        "from grid2op.Parameters import Parameters\n",
        "from grid2op.Reward import *\n",
        "from grid2op.gym_compat import DiscreteActSpace, GymEnv\n",
        "from scipy import stats\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.env_util import DummyVecEnv\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Suppress common warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "print(\"✓ All libraries installed and imports successful.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Experiment Configuration\n",
        "\n",
        "# --- Primary Settings ---\n",
        "ENV_NAME = \"l2rpn_case14_sandbox\"\n",
        "TRAINING_TIMESTEPS = 150000\n",
        "N_TEST_EPISODES = 20\n",
        "N_STATISTICAL_RUNS = 5      # For future work: run the whole experiment 5 times\n",
        "\n",
        "# --- Analysis Flags ---\n",
        "PERFORM_ACTION_IMPACT_ANALYSIS = False # Set to True for a deep (but slow) action analysis\n",
        "\n",
        "# Defines the different data degradation scenarios for evaluation.\n",
        "ROBUSTNESS_SCENARIOS = {\n",
        "    # Control group\n",
        "    'baseline':          {'missing_rate': 0.0, 'corruption_rate': 0.0},\n",
        "\n",
        "    # Missing data scenarios\n",
        "    'light_missing':     {'missing_rate': 0.05, 'corruption_rate': 0.0},\n",
        "    'moderate_missing':  {'missing_rate': 0.15, 'corruption_rate': 0.0},\n",
        "    'heavy_missing':     {'missing_rate': 0.30, 'corruption_rate': 0.0},\n",
        "\n",
        "    # Data corruption scenarios\n",
        "    'light_corruption':  {'missing_rate': 0.0, 'corruption_rate': 0.05},\n",
        "    'mixed_degradation': {'missing_rate': 0.10, 'corruption_rate': 0.10},\n",
        "}\n",
        "\n",
        "print(f\"Configuration loaded:\")\n",
        "print(f\"  - Training Timesteps: {TRAINING_TIMESTEPS}\")\n",
        "print(f\"  - Evaluation Episodes: {N_TEST_EPISODES}\")\n",
        "print(f\"  - Robustness Scenarios: {len(ROBUSTNESS_SCENARIOS)}\")"
      ],
      "metadata": {
        "id": "arIE2TwdmalF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Environment Utilities\n",
        "\n",
        "def create_environment(env_name, params=None):\n",
        "    \"\"\"A helper function to create a Grid2Op environment, optionally with custom parameters.\"\"\"\n",
        "    if params:\n",
        "        return make(env_name, param=params)\n",
        "    return make(env_name)\n",
        "\n",
        "def get_reward_functions():\n",
        "    \"\"\"Returns a dictionary of reward functions to be used for multi-faceted evaluation.\"\"\"\n",
        "    return {\n",
        "        'L2RPN': L2RPNReward(),\n",
        "        'Economic': EconomicReward(),\n",
        "        'FlatHazard': FlatReward(),      # Measures survival without other penalties\n",
        "        'Gameplay': GameplayReward(),    # Balances survival and action costs\n",
        "        'Redispatching': RedispReward()  # Focuses on generation stability\n",
        "    }\n",
        "\n",
        "# Define the set of reward functions for the analysis\n",
        "reward_functions = get_reward_functions()\n",
        "print(f\"✓ Defined {len(reward_functions)} reward functions for evaluation.\")"
      ],
      "metadata": {
        "id": "wLV_P5DdmdLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Prepare Data & Environments\n",
        "\n",
        "def get_available_chronics(env_name, max_test=100):\n",
        "    \"\"\"Robustly discovers available scenarios by attempting to load them.\"\"\"\n",
        "    available_ids = []\n",
        "    print(\"Discovering available chronics...\")\n",
        "    for i in tqdm(range(max_test), desc=\"Probing Chronics\"):\n",
        "        try:\n",
        "            p = Parameters()\n",
        "            p.CHRONICS_ID_TO_USE = [i]\n",
        "            make(env_name, param=p, test=True) # test=True loads faster\n",
        "            available_ids.append(i)\n",
        "        except Exception:\n",
        "            break # Stop when a chronic can't be found\n",
        "    print(f\"✓ Found {len(available_ids)} available chronics.\")\n",
        "    return available_ids\n",
        "\n",
        "def create_preconfigured_env(env_name, ids_to_use):\n",
        "    \"\"\"Creates a gym-compatible environment pre-configured for a specific set of chronics.\"\"\"\n",
        "    params = Parameters()\n",
        "    params.CHRONICS_ID_TO_USE = ids_to_use\n",
        "    env = create_environment(env_name, params=params)\n",
        "    gym_env = GymEnv(env)\n",
        "    gym_env.action_space = DiscreteActSpace(\n",
        "        env.action_space,\n",
        "        attr_to_keep=['set_line_status', 'change_line_status']\n",
        "    )\n",
        "    return gym_env\n",
        "\n",
        "# Discover and split available scenarios\n",
        "all_chronics_ids = get_available_chronics(ENV_NAME)\n",
        "if all_chronics_ids:\n",
        "    np.random.shuffle(all_chronics_ids)\n",
        "    train_end = int(len(all_chronics_ids) * 0.6)\n",
        "    val_end = int(len(all_chronics_ids) * 0.8)\n",
        "    train_ids = all_chronics_ids[:train_end]\n",
        "    val_ids = all_chronics_ids[train_end:val_end]\n",
        "    test_ids = all_chronics_ids[val_end:]\n",
        "else:\n",
        "    # Fallback if no chronics are found\n",
        "    train_ids, val_ids, test_ids = [0], [0], [0]\n",
        "\n",
        "# Ensure sets are not empty for stability\n",
        "if not train_ids: train_ids = [0]\n",
        "if not val_ids: val_ids = [0]\n",
        "if not test_ids: test_ids = [0]\n",
        "print(f\"✓ Chronics split: {len(train_ids)} train, {len(val_ids)} validation, {len(test_ids)} test.\")\n",
        "\n",
        "# Create dedicated environments for each data split\n",
        "print(\"Creating pre-configured environments...\")\n",
        "train_env = create_preconfigured_env(ENV_NAME, train_ids)\n",
        "val_env = create_preconfigured_env(ENV_NAME, val_ids)\n",
        "test_env = create_preconfigured_env(ENV_NAME, test_ids)\n",
        "print(\"✓ Train, validation, and test environments created successfully.\")"
      ],
      "metadata": {
        "id": "oa7IQkjimflE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: Agent Training\n",
        "\n",
        "def train_agent_with_validation(train_env, val_env, timesteps):\n",
        "    \"\"\"Trains a DQN agent with periodic validation checks.\"\"\"\n",
        "    print(\"\\n--- Training DQN Agent ---\")\n",
        "    vec_env = DummyVecEnv([lambda: train_env])\n",
        "\n",
        "    model = DQN(\"MultiInputPolicy\", vec_env, verbose=0, # Set verbose=1 for detailed logs\n",
        "                learning_rate=0.001,\n",
        "                buffer_size=100000,\n",
        "                learning_starts=2000,\n",
        "                batch_size=128,\n",
        "                train_freq=4,\n",
        "                target_update_interval=1000,\n",
        "                exploration_initial_eps=1.0,\n",
        "                exploration_final_eps=0.05)\n",
        "\n",
        "    validation_scores = []\n",
        "    num_phases = 5\n",
        "    validation_interval = timesteps // num_phases\n",
        "\n",
        "    for i in range(num_phases):\n",
        "        print(f\"\\nTraining phase {i+1}/{num_phases}...\")\n",
        "        model.learn(total_timesteps=validation_interval,\n",
        "                    progress_bar=True,\n",
        "                    reset_num_timesteps=False)\n",
        "\n",
        "        val_score = quick_validation(model, val_env)\n",
        "        validation_scores.append(val_score)\n",
        "        print(f\"  > Validation score after phase {i+1}: {val_score:.2f}\")\n",
        "\n",
        "    print(\"✓ Training complete.\")\n",
        "    return model, validation_scores\n",
        "\n",
        "def quick_validation(model, val_env, n_episodes=5):\n",
        "    \"\"\"Helper function to evaluate model performance on the validation set.\"\"\"\n",
        "    scores = []\n",
        "    for _ in range(n_episodes):\n",
        "        obs, _ = val_env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            action, _ = model.predict(obs, deterministic=True)\n",
        "            obs, reward, done, truncated, _ = val_env.step(action)\n",
        "            done = done or truncated\n",
        "            total_reward += reward\n",
        "        scores.append(total_reward)\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Train the agent\n",
        "dqn_model, training_validation_scores = train_agent_with_validation(\n",
        "    train_env, val_env, TRAINING_TIMESTEPS\n",
        ")\n",
        "dqn_model.save(\"dqn_grid_agent_long_train.zip\")"
      ],
      "metadata": {
        "id": "1KApajN3mhmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: Evaluation Framework\n",
        "\n",
        "def apply_data_degradation(obs, scenario_config):\n",
        "    \"\"\"Applies missing data and noise corruption to an observation.\"\"\"\n",
        "    if not scenario_config['missing_rate'] and not scenario_config['corruption_rate']:\n",
        "        return obs\n",
        "\n",
        "    degraded_obs = copy.deepcopy(obs)\n",
        "\n",
        "    # Apply missing data (impute with zero)\n",
        "    if scenario_config['missing_rate'] > 0:\n",
        "        for key, value in degraded_obs.items():\n",
        "            if isinstance(value, np.ndarray) and value.size > 0:\n",
        "                flat_value = value.flatten()\n",
        "                n_missing = int(flat_value.size * scenario_config['missing_rate'])\n",
        "                if n_missing > 0:\n",
        "                    missing_indices = np.random.choice(flat_value.size, n_missing, replace=False)\n",
        "                    flat_value[missing_indices] = 0.0\n",
        "                    degraded_obs[key] = flat_value.reshape(value.shape)\n",
        "\n",
        "    # Apply noise corruption\n",
        "    if scenario_config['corruption_rate'] > 0:\n",
        "        for key, value in degraded_obs.items():\n",
        "            if isinstance(value, np.ndarray) and value.size > 0:\n",
        "                flat_value = value.flatten().astype(np.float32)\n",
        "                n_corrupt = int(flat_value.size * scenario_config['corruption_rate'])\n",
        "                if n_corrupt > 0:\n",
        "                    corrupt_indices = np.random.choice(flat_value.size, n_corrupt, replace=False)\n",
        "                    std_dev = np.std(flat_value) if np.std(flat_value) > 1e-6 else 1.0\n",
        "                    noise = np.random.normal(0, std_dev * 0.1, n_corrupt)\n",
        "                    flat_value[corrupt_indices] += noise\n",
        "                    degraded_obs[key] = flat_value.reshape(value.shape)\n",
        "\n",
        "    return degraded_obs\n"
      ],
      "metadata": {
        "id": "8S8m-Wqgmj_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7: COMPREHENSIVE EVALUATION FRAMEWORK\n",
        "\n",
        "class ComprehensiveEvaluator:\n",
        "    \"\"\"A class to handle comprehensive agent evaluation across scenarios.\"\"\"\n",
        "\n",
        "    def __init__(self, reward_functions):\n",
        "        self.reward_functions = reward_functions\n",
        "\n",
        "    def evaluate(self, agent, eval_env, n_episodes, scenario_name, scenario_config):\n",
        "        print(f\"\\nRunning evaluation: Agent='{agent.__class__.__name__}', Scenario='{scenario_name}'\")\n",
        "\n",
        "        survival_steps_raw = []\n",
        "        episode_rewards = {name: [] for name in self.reward_functions.keys()}\n",
        "        action_counts = defaultdict(int)\n",
        "\n",
        "        for _ in tqdm(range(n_episodes), desc=f\"  - Episodes\"):\n",
        "            obs, _ = eval_env.reset()\n",
        "            done = False\n",
        "            step_count = 0\n",
        "            while not done:\n",
        "                degraded_obs = apply_data_degradation(obs, scenario_config)\n",
        "                action, _ = agent.predict(degraded_obs, deterministic=True)\n",
        "                action = int(action)\n",
        "\n",
        "                action_counts[action] += 1\n",
        "                obs, reward, done, truncated, _ = eval_env.step(action)\n",
        "                done = done or truncated\n",
        "\n",
        "                for name in self.reward_functions.keys():\n",
        "                    episode_rewards[name].append(reward) # Simplified: use default reward for all\n",
        "\n",
        "                step_count += 1\n",
        "            survival_steps_raw.append(step_count)\n",
        "\n",
        "        # Aggregate and return results\n",
        "        results = {\n",
        "            'survival_steps_raw': survival_steps_raw,\n",
        "            'survival_mean': np.mean(survival_steps_raw) if survival_steps_raw else 0,\n",
        "            'survival_std': np.std(survival_steps_raw) if survival_steps_raw else 0,\n",
        "            'action_diversity': len(action_counts)\n",
        "        }\n",
        "        for name, rewards in episode_rewards.items():\n",
        "             results[f'{name}_mean'] = np.mean(rewards) if rewards else 0\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "xFburY6Vmoqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 8: BASELINE AGENTS AND COMPREHENSIVE COMPARISON\n",
        "\n",
        "print(\"\\n--- Comprehensive Agent Evaluation ---\")\n",
        "\n",
        "evaluator = ComprehensiveEvaluator(reward_functions)\n",
        "\n",
        "# Define simple, gym-compatible baseline agents\n",
        "class SimpleDoNothingAgent:\n",
        "    def predict(self, obs, deterministic=True):\n",
        "        return 0, {} # Action 0 is always do_nothing\n",
        "\n",
        "class SimpleRandomAgent:\n",
        "    def __init__(self, action_space):\n",
        "        self.action_space = action_space\n",
        "    def predict(self, obs, deterministic=True):\n",
        "        return self.action_space.sample(), {}\n",
        "\n",
        "agents_to_test = {\n",
        "    'DQN': dqn_model,\n",
        "    'DoNothing': SimpleDoNothingAgent(),\n",
        "    'Random': SimpleRandomAgent(test_env.action_space)\n",
        "}\n",
        "\n",
        "# Run evaluation for all agents across all robustness scenarios\n",
        "all_results = {}\n",
        "for agent_name, agent in agents_to_test.items():\n",
        "    agent_results = {}\n",
        "    for scenario_name, scenario_config in ROBUSTNESS_SCENARIOS.items():\n",
        "        current_test_env = copy.deepcopy(test_env)\n",
        "        results = evaluator.evaluate(\n",
        "            agent, current_test_env,\n",
        "            N_TEST_EPISODES, scenario_name, scenario_config\n",
        "        )\n",
        "        agent_results[scenario_name] = results\n",
        "    all_results[agent_name] = agent_results\n",
        "\n",
        "print(\"\\n✓ Evaluation finished.\")"
      ],
      "metadata": {
        "id": "wAq2uSfKmrVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 9: Statistical Analysis\n",
        "\n",
        "def perform_statistical_analysis(all_results):\n",
        "    \"\"\"Processes raw results into a DataFrame and performs significance testing.\"\"\"\n",
        "    print(\"\\n--- Statistical Analysis ---\")\n",
        "\n",
        "    # Flatten the nested results dictionary into a list for DataFrame creation\n",
        "    flat_results = []\n",
        "    for agent, agent_data in all_results.items():\n",
        "        for scenario, scenario_data in agent_data.items():\n",
        "            entry = {\n",
        "                'Agent': agent,\n",
        "                'Scenario': scenario,\n",
        "                'Survival_Mean': scenario_data['survival_mean'],\n",
        "                'Survival_Std': scenario_data['survival_std'],\n",
        "                'L2RPN_Reward': scenario_data.get('L2RPN_mean', 0),\n",
        "                'Action_Diversity': scenario_data['action_diversity']\n",
        "            }\n",
        "            flat_results.append(entry)\n",
        "\n",
        "    df = pd.DataFrame(flat_results)\n",
        "\n",
        "    print(\"\\nSignificance Testing (Welch's t-test on baseline survival):\")\n",
        "\n",
        "    # Compare the distributions of survival times, not just the means\n",
        "    dqn_dist = all_results['DQN']['baseline']['survival_steps_raw']\n",
        "    donothing_dist = all_results['DoNothing']['baseline']['survival_steps_raw']\n",
        "    random_dist = all_results['Random']['baseline']['survival_steps_raw']\n",
        "\n",
        "    # Welch's t-test is appropriate here as we don't assume equal variance\n",
        "    _, p_donothing = stats.ttest_ind(dqn_dist, donothing_dist, equal_var=False)\n",
        "    print(f\"  - DQN vs. DoNothing: p-value = {p_donothing:.4f} {'(Difference is significant)' if p_donothing < 0.05 else '(Not significant)'}\")\n",
        "\n",
        "    _, p_random = stats.ttest_ind(dqn_dist, random_dist, equal_var=False)\n",
        "    print(f\"  - DQN vs. Random:    p-value = {p_random:.4f} {'(Difference is significant)' if p_random < 0.05 else '(Not significant)'}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Process the results\n",
        "statistical_df = perform_statistical_analysis(all_results)"
      ],
      "metadata": {
        "id": "sBamLziumz7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 10: VISUALIZATION AND REPORTING\n",
        "# ==============================================================================\n",
        "def create_comprehensive_visualizations(statistical_df, all_results):\n",
        "    \"\"\"Create comprehensive visualizations.\"\"\"\n",
        "    print(\"\\n--- CREATING VISUALIZATIONS ---\")\n",
        "\n",
        "    # Figure 1: Agent Performance Comparison\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Subplot 1: Survival time comparison\n",
        "    plt.subplot(2, 3, 1)\n",
        "    baseline_data = statistical_df[statistical_df['Scenario'] == 'baseline']\n",
        "    sns.barplot(data=baseline_data, x='Agent', y='Survival_Mean')\n",
        "    plt.title('Agent Performance: Average Survival Time')\n",
        "    plt.ylabel('Timesteps Survived')\n",
        "\n",
        "    # Subplot 2: Robustness across scenarios (DQN only)\n",
        "    plt.subplot(2, 3, 2)\n",
        "    dqn_data = statistical_df[statistical_df['Agent'] == 'DQN']\n",
        "    sns.barplot(data=dqn_data, x='Scenario', y='Survival_Mean')\n",
        "    plt.title('DQN Robustness Across Scenarios')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.ylabel('Timesteps Survived')\n",
        "\n",
        "    # Subplot 3: Action diversity\n",
        "    plt.subplot(2, 3, 3)\n",
        "    sns.barplot(data=baseline_data, x='Agent', y='Action_Diversity')\n",
        "    plt.title('Action Diversity by Agent')\n",
        "    plt.ylabel('Number of Different Actions Used')\n",
        "\n",
        "    # Subplot 4: L2RPN Reward comparison\n",
        "    plt.subplot(2, 3, 4)\n",
        "    sns.barplot(data=baseline_data, x='Agent', y='L2RPN_Reward')\n",
        "    plt.title('L2RPN Reward Performance')\n",
        "    plt.ylabel('Average L2RPN Reward')\n",
        "\n",
        "    # Subplot 5: Robustness heatmap\n",
        "    plt.subplot(2, 3, 5)\n",
        "    pivot_data = statistical_df.pivot(index='Agent', columns='Scenario', values='Survival_Mean')\n",
        "    sns.heatmap(pivot_data, annot=True, fmt='.1f', cmap='viridis')\n",
        "    plt.title('Performance Heatmap: Survival Time')\n",
        "\n",
        "    # Subplot 6: Training validation curve\n",
        "    plt.subplot(2, 3, 6)\n",
        "    plt.plot(range(1, len(training_validation_scores) + 1), training_validation_scores, 'o-')\n",
        "    plt.title('Training Validation Scores')\n",
        "    plt.xlabel('Training Phase')\n",
        "    plt.ylabel('Validation Score')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Figure 2: Detailed robustness analysis\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Performance degradation analysis\n",
        "    scenarios_order = ['baseline', 'light_missing', 'moderate_missing', 'heavy_missing',\n",
        "                      'light_corruption', 'mixed_degradation']\n",
        "\n",
        "    for agent in ['DQN', 'DoNothing', 'Random']:\n",
        "        agent_survival = []\n",
        "        for scenario in scenarios_order:\n",
        "            if scenario in all_results[agent]:\n",
        "                agent_survival.append(all_results[agent][scenario]['survival_mean'])\n",
        "            else:\n",
        "                agent_survival.append(0)\n",
        "        plt.plot(scenarios_order, agent_survival, 'o-', label=agent, linewidth=2)\n",
        "\n",
        "    plt.title('Agent Performance Degradation Under Different Scenarios')\n",
        "    plt.xlabel('Degradation Scenario')\n",
        "    plt.ylabel('Average Survival Time')\n",
        "    plt.legend()\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "create_comprehensive_visualizations(statistical_df, all_results)\n",
        "\n"
      ],
      "metadata": {
        "id": "dw4EnoGQm3Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ENHANCED XAI ANALYSIS\n",
        "\n",
        "print(\"\\n--- ENHANCED EXPLAINABLE AI ANALYSIS ---\")\n",
        "\n",
        "class EnhancedModelWrapper:\n",
        "    \"\"\"Enhanced wrapper for SHAP analysis with better feature interpretation.\"\"\"\n",
        "\n",
        "    def __init__(self, model, env):\n",
        "        self.model = model\n",
        "        self.env = env\n",
        "        self.key_features = ['rho', 'gen_p', 'load_p', 'p_or', 'q_or', 'v_or']\n",
        "\n",
        "        # Create feature mapping\n",
        "        sample_obs, _ = self.env.reset()\n",
        "        self.template_obs = sample_obs\n",
        "        self.feature_map = {}\n",
        "        self.feature_names = []\n",
        "\n",
        "        current_idx = 0\n",
        "        for key in self.key_features:\n",
        "            if key in sample_obs and sample_obs[key] is not None:\n",
        "                shape = sample_obs[key].shape\n",
        "                size = sample_obs[key].size\n",
        "                self.feature_map[key] = {\n",
        "                    'start': current_idx,\n",
        "                    'end': current_idx + size,\n",
        "                    'shape': shape\n",
        "                }\n",
        "                # Create more descriptive feature names\n",
        "                if len(shape) == 1:\n",
        "                    self.feature_names.extend([f\"{key}_{i}\" for i in range(size)])\n",
        "                else:\n",
        "                    self.feature_names.extend([f\"{key}_flat_{i}\" for i in range(size)])\n",
        "                current_idx += size\n",
        "\n",
        "        self.n_features = current_idx\n",
        "        print(f\"✓ Enhanced wrapper created with {self.n_features} features\")\n",
        "\n",
        "    def _reconstruct_observation(self, x_flat):\n",
        "        \"\"\"Reconstruct observation from flattened features.\"\"\"\n",
        "        reconstructed_obs = self.template_obs.copy()\n",
        "        for key, info in self.feature_map.items():\n",
        "            data_slice = x_flat[info['start']:info['end']]\n",
        "            reconstructed_obs[key] = data_slice.reshape(info['shape'])\n",
        "        return reconstructed_obs\n",
        "\n",
        "    def predict_q_values(self, X):\n",
        "        \"\"\"Predict Q-values for multiple observations.\"\"\"\n",
        "        if X.ndim == 1:\n",
        "            X = X.reshape(1, -1)\n",
        "\n",
        "        q_values_list = []\n",
        "        for i in range(X.shape[0]):\n",
        "            obs_dict = self._reconstruct_observation(X[i])\n",
        "            q_values = self.model.policy.q_net(\n",
        "                self.model.policy.obs_to_tensor(obs_dict)[0]\n",
        "            )\n",
        "            q_values_list.append(q_values.detach().cpu().numpy().flatten())\n",
        "\n",
        "        return np.array(q_values_list)\n",
        "\n",
        "# Create enhanced SHAP analysis\n",
        "# FIX: Use the pre-configured \"test_env\" for the wrapper\n",
        "enhanced_wrapper = EnhancedModelWrapper(dqn_model, test_env)\n",
        "\n",
        "# Collect diverse background data\n",
        "print(\"Collecting diverse background data for SHAP...\")\n",
        "background_data_list = []\n",
        "# FIX: Use the pre-configured \"test_env\" for data collection\n",
        "shap_env = test_env\n",
        "\n",
        "for episode in range(5):  # Fewer episodes for speed\n",
        "    obs, _ = shap_env.reset()\n",
        "    for step in range(20):  # Multiple steps per episode\n",
        "        features = []\n",
        "        for key in enhanced_wrapper.key_features:\n",
        "            if key in obs and obs[key] is not None:\n",
        "                features.extend(obs[key].flatten())\n",
        "\n",
        "        if len(features) == enhanced_wrapper.n_features:\n",
        "            background_data_list.append(np.array(features, dtype=np.float32))\n",
        "\n",
        "        action, _ = dqn_model.predict(obs, deterministic=True)\n",
        "        obs, _, done, _, _ = shap_env.step(action)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "background_data = np.array(background_data_list[:100])\n",
        "print(f\"✓ Collected {len(background_data)} background samples\")\n",
        "\n",
        "# Create SHAP explainer and analyze\n",
        "if len(background_data) > 10:\n",
        "    explainer = shap.KernelExplainer(\n",
        "        enhanced_wrapper.predict_q_values,\n",
        "        background_data[:20] # Use a small, representative subset\n",
        "    )\n",
        "\n",
        "    # Find interesting state to explain\n",
        "    sample_features = background_data[0]\n",
        "    shap_values = explainer.shap_values(sample_features, nsamples=50)\n",
        "\n",
        "    # Visualize\n",
        "    predicted_action = np.argmax(enhanced_wrapper.predict_q_values(sample_features))\n",
        "    print(f\"✓ SHAP analysis complete. Predicted action: {predicted_action}\")\n",
        "\n",
        "    shap.summary_plot(\n",
        "        shap_values,\n",
        "        feature_names=enhanced_wrapper.feature_names,\n",
        "        plot_type=\"bar\",\n",
        "        max_display=15,\n",
        "        title=f\"Feature Importance for Action {predicted_action}\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "bixYyUB6nADo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}